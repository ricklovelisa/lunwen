文本分类研究

3 文本分类研究
3.1 文本分类概述
	文本分类指对大量文本根据某一维度，对其文本内容进行分类的过程。
	在机器学习兴起之前，文本分类工作主要有人工完成。例如路透社的专家分类。但是人工进行文本分类效率低下，并且错误率较高。在机器学习兴起之后，人们开始探索如何让机器代替人工进行繁重的分类任务，自动化文本分类诞生了。
	目前英文的自动化文本分类已经取得了丰富的成果，并且提出了许多算用于自动化文本分类。比较经典的有贝叶斯分类器、SVM分类器、KNN分类器、神经网络分类器等。
	

3.2 文本分类任务
	文本分类任务简单的描述为如下映射：
	A -> B
	A为未标注类别的输入文本集合，B为输出类别集合又称A在对应类别中的映射，->为分类器分类过程。

3.3 文本分类过程
	文本分类器训练流程如下：

	原始训练文本 -> 文本预处理 -> 文本结构化 -> [文本分类器训练 -> 分类器模型参数调优] -> 分类器性能测试并择优选取 -> 分类器工程化并集成至指定系统 -> 模型日常维护

	文本分类器分类流程如下
	未标注的待分类文本 -> 文本预处理 -> 文本结构化 -> 输入分类器分类 -> 输出类别标注


	##########################################################################################
	原始训练文本 -> 文本清理 -> 分词/去停用词 -> 特征选择 -> 权重计算 -> 构建空间向量模型 -> [文本分类器训练 -> 分类器模型参数调优] -> 分类器性能测试并择优选取 -> 分类器工程化并集成至指定系统 -> 模型日常维护

	文本分类器分类流程如下
	未标注的待分类文本 -> 文本清理 -> 分词/去停用词 -> 特征选择 -> 权重计算 -> 构建空间响亮模型 -> 输入分类器分类 -> 输入类别标注
	##########################################################################################


	3.3.1 文本结构化
		  文本结构化又称文本向量化，是文本分类任务中最为重要的一个环节。
		  文本结构化的目的是让非结构化的文本数据在尽可能的减少信息损失的情况下转化为计算机可以识别并计算的数据格式。一下将对文本结构化过程中的几个重要步骤进行展开。

		  3.3.1.1 文本表示
		  		  文本结构化最重要的目标就是将文本表示为计算机可以理解并且运算的数据形式。
		  		  现在主要有三种文本表示模型：
		  		  a.概率模型
		  		  b.代数论模型
		  		  c.集合论模型

		  		  其中代数论中的向量空间模型是应用最广泛的文本表示模型。其由XXXX#需要提供参考文献#提出，并且广泛应用于信息检索领域，并且取得了不错的效果。		  		
		  
		  3.3.1.2 最小语言特征单元提取
		  		  
		  		  最小语言特征单元（以下简称特征单元）提取主要要目的是从文本中抽取出尽可能小，包含尽可能多信息的粒度单元。在中文文本处理中，这个过程又称为分词。

		  		  在文本挖掘的兴起的英语世界，特征单元字面上就是英文单词，与之所对应的是中文的词，而中文不像英语每个单词都是独立不可再分的，换而言之相对与英文单词，中文的单词没有明确的边界。

		  		  中文中特征单元粒度最小的是单字，虽然其足够小，能够最大程度上减小稀疏数据的产生，但是其所包含的语言信息太少，并不符合上述分词的要求。但是近期也有国外学者对中文文本分类做了部分研究，提出了根据单字构造空间向量模型，并且将其应用至亚马逊买家评论以及文本分类，并提高了准确度[1]《Text Understanding from Scratch》arXiv:1502.01710v1 [cs.LG] 5 Feb 2015。

		  		  值得一提的是，李景阳[2]在《文本分类中的特征选择与权重计算》中提到，特征单元维数在50000以上时，二字串的分词性能更加优良。但是考虑本文的应用场景是针对新闻资讯系统，每天更新的文章数量有限，相对于样本数量，维数过大会造成统计学中的维度灾难，同时会造成模型的过拟合，反而会降低分类器的性能。

		  		  现有的中文分词工具有如下：

		  		  ##艳萍做的表格##

		  		  本文对分词不做过多讨论，使用的分词工具是基于jieba分词的jiebaR。

		  		  分词得到的语言最小单元在一般情况下，数量为会非常的大。其中包含了大量的噪音，在之后的工作之前需要对分词结果进行一轮粗略的清理，目的是清除例如“的”这类广泛存在于文本中，却又没有实际含义的词。这类词被称作为停用词。现如今已经有许多前人总结出了中文中的一些停用次，本文中用到的停用词整合了哈工大停用词表、四川大学机器智能实验室停用词库、百度停用词表。

		  3.3.1.3 特征选择
		  		  特征选择是对分词结果进行降维的过程，在分词并且去除停用词之后进行。一般来说，每一个分类器都需要进行特征选择，目的是进一步减少特征单元的数量，降低计算压力，提升分类器性能。

		  		  一般来说有如下特征选择的方法：
		  		  a.文档频度(df)
		  		  	文档频度是最为传统的特征选择方法，但是大量实验表明，文档频度有着出人意料的效果，对于提升分类器性能，降低计算压力效果较好。本文将采取df > 2的方式对语料统一截断并降维。
		  		  b.卡方检验(chisq test)
		  		  	卡方检验源自统计学，其意义是度量某一变量与另一变量的相关程度，是为显著性检验。其一般形式计算公式为：
		  		  		# 数学公式 #
		  		  	由于其同时考虑的正例和负例，具有较强的现实意义。并且其是特征选择性能最为优良的指标之一。《文本分类中的特征选择与权重计算》[1,48,52]
		  		  c.互信息
		  		  	互信息源于信息论，描述了一个随机变量中包含的关于另一个随机变量的信息量这样一个指标。其数学公式为：
		  		  		# 数学公式 #
		  		  	互信息在文本特征为度筛选是也有广泛的应用，在本文中，不做考虑。
		  		  d.信息增益(information gain)
		  		  	信息增益同样来自信息论。其最典型的特征是非对称的，即P对Q的增益量Q不等于Q对P的增益量。其数学公式如下：
		  		  		# 数学公式 #
		  		  	广泛的实验证明，IG同样是性能良好的特征维度删选指标之一，但较为偏向高频特征。《文本分类中的特征选择与权重计算》[1,52]
		  3.3.1.4 权重计算
		  		  常用的权重计算方式有三种:
		  		  a.布尔型
		  		  	布尔型是最基础的权重表示方式。如果类别ci存在特征ti，则wcti为1，否则，wcti为0。由于布尔权重会丢失大量的文本信息，所以在本文那种不对其作过多的描述和进一步的实验。
		  		  b.频度。词频(tf)
		  		  	词频是最常用的权重计算方式之一，源自统计学，定义为一个词在一篇文档中出现的频数。其认为，一个词在一篇文档中出现的频数越高，则其重要程度越重。由于计算方便，以及优良的分类性能，成为了工程实践中分类模型权重的不二之选。
		  		  c.TF-IDF（term frequency–inverse document frequency）
		  		  	TF-IDF源自于信息检索，描述词频高的词对于一个语料库的其中一个文件的重要程度。数学公式为：
		  		  		# 数学公式 #
		  		  	很明显，TF-IDF由两部分组成，词频tf和逆文档频度idf相乘。tf在b.已经讨论过，而idf主要描述一个词在逆向文档中的频度，主要考量一个词在一个特定的预料库中的重要程度。一般来说，词的TF越大则说明该词越重要，而IDF越大说明在语料库中该词分布月普遍，相对的重要性越低。在本文中，TF-IDF指标由于其特殊性——需要对整个预料进行计算，导致了工程实施上的障碍。对于新获取的待分类的文章，其中词汇的TF-IDF，应该与训练集中的词一同计算才能保证其准缺性，这样就导致了一个问题——每一次分类之前都需要重新计算一边权重，接着影响到模型的训练，这样会严重制约分类效率，并且不利于模型维护以及更新。所以在本文中，TF-IDF权重只会作为一个重要的实验参考对象，来评判其他权重的分类性能。



3.4 文本分类算法
	文本分类的算法模型大多基于传统的分类模型。在本问提到的新闻资讯系统中，类别是认为指定并且确定的。所以采用的是有监督的机器学习算法。
	从概率模型的角度来说，常用的有监督的机器学习算法主要有两种：
		a.基于概率的产生式模型(generative model)[《文本分类中的特征选择与权重计算》]，其主要代表有朴素贝叶斯模型，高斯模型，贝叶斯网络等，限于本文的应用场景，除了朴素贝叶斯模型外，其他模型不做过多的阐述和介绍。
		朴素贝叶斯模型已著名的贝叶斯定理为基础，在文本分类中可以描述为
		P( Category | Document) = P ( Document | Category ) * P( Category) / P(Document)
		遵循经验风险最小原则，其鲜明的特点是运用了：先验知识（先验概率）以及模型可以增量式学习[《文本分类中的特征选择与权重计算》]，但是由于贝叶斯定理的一个前提假设，变量之间必须满足独立性原则，使其在文本挖掘中缺乏足够的理论支持。因为文本中的词不是相互独立的，其间包含了语义关系。同时，基于贝叶斯理论的模型需求大量的训练集，以寻求更加逼近真实的概率分布。在少量样本的的情况下，其分类性能会大幅衰减。但是在实际使用中，朴素贝叶斯的效果是值得称道的。

		b.判别模型，也称非参数模型。其理论基础源自于统计学习理论，提出了结构风险最小的思想。典型代表是SVM，逻辑回归等。本文将着重对SVM进行阐述和分析，并且在实验中作为被试，对比如其他分类算法。

		SVM模型在上个世纪提出，其思想为将现有维度下不可分的问题映射到高维再进行分类，从而解决分类问题。其中区分类别的向量，被称为支持向量。#SVM分类图例一张#
		SVM的模型
		SVM的理论背景较为深刻，而在本文的背景下，更多的是应用场景，对其理论描述不做过多的讨论。


3.5 文本分类性能指标
	文本分类指标参考的是信息论中的命中率。
	用2X2矩阵表示为
	# 命中矩阵 #
3.6 小结
	本文主要是对SVM、贝叶斯、传统分类模型的比较，以及将最优算法的工程化实现。